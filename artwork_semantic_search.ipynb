{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eb090d9-896e-43f8-9df6-c6d58f17790e",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "---\n",
    "\n",
    "## Semantic search and query expansion\n",
    "\n",
    "This notebook contains code to create a simple search engine that allows users to identify an artwork which matches their query based on a natural-language description of the artwork, as well as its title. Artwork information is from [WikiArt](https://www.wikiart.org/). This code makes use of [Whoosh](https://whoosh.readthedocs.io/en/latest/intro.html), an open-source Python search-engine library, as the framework for the search engine and carries out query-term expansion based on [pre-trained word embeddings from the Nordic Language Processing Laboratory \\(NLPL\\)](http://vectors.nlpl.eu/repository/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0457e937-b4e3-4708-98a0-4f401308ac1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hannahtempest/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hannahtempest/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import wordnet\n",
    "nltk.download('wordnet') # large lexical database\n",
    "#nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "from whoosh.fields import Schema, TEXT, ID, STORED\n",
    "from whoosh import index\n",
    "from whoosh.qparser import MultifieldParser\n",
    "#from whoosh.reading import IndexReader\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aeb7dc-860b-4446-b03c-ea49e67deafd",
   "metadata": {},
   "source": [
    "## Set up language model for query-term expansion\n",
    "\n",
    "This code uses the following pre-trained word embedding model:\n",
    "\n",
    "|   |   |\n",
    "|---|---|\n",
    "| ID: | 12 |\n",
    "| Download link:| http://vectors.nlpl.eu/repository/20/12.zip |\n",
    "| Vector size: | 300 |\n",
    "| Window: | 5 |\n",
    "| Corpus: | Gigaword 5th Edition |\n",
    "| Vocabulary size: | 292,479 |\n",
    "| Algorithm: | Gensim Continuous Skipgram |\n",
    "| Lemmatization: | False |\n",
    "\n",
    "The model has been downloaded from the [online repository](http://vectors.nlpl.eu/repository/) and saved locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be9146d7-1350-4831-b89d-883bca348d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set model path\n",
    "word_embeddings_model_path = '12/model.bin'\n",
    "\n",
    "# Set search-engine path\n",
    "search_engine_index_path = 'whoosh_artwork_index'\n",
    "\n",
    "# Define a function to load embeddings from pretrained model\n",
    "def load_model(path):\n",
    "    word_embeddings_model = KeyedVectors.load_word2vec_format(word_embeddings_model_path, binary=True)\n",
    "    return word_embeddings_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "690fb0f8-096a-4d94-bc55-e6dd8d0b27f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the above function to load the model\n",
    "model = load_model(word_embeddings_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14bf8860-441c-4132-bdcf-90bee90eaab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to use the language model to get synonyms for a given term\n",
    "def get_similar_words(model, search_term):\n",
    "    similarity_list = model.most_similar(search_term, topn=5)\n",
    "    similar_words = [sim_tuple[0] for sim_tuple in similarity_list]\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c9458a-a2ed-41f4-91a2-df821756aad5",
   "metadata": {},
   "source": [
    "### Get artwork information\n",
    "\n",
    "The artworks included in the search engine are the most-searched artworks on WikiArt from the last 30 days, so they will change periodically. Artwork information is gathered using the [WikiArt API](https://www.wikiart.org/en/App/GetApi), whihc returns information in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0756f8f3-bb6c-4409-b5de-1d6473c80cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The 600 most-viewed paintings on Wiki Art from the last 30 days\n",
    "# Store the URL in `url` as parameter for urlopen\n",
    "url = \"https://www.wikiart.org/en/App/Painting/MostViewedPaintings?randomSeed=123&json=2&inPublicDomain={true/false}\"\n",
    "  \n",
    "# Store the data from urlopen() call to URL as a new variable called `response`\n",
    "response = urlopen(url)\n",
    "  \n",
    "# Use JSON to load the data\n",
    "data = json.loads(response.read())\n",
    "\n",
    "# Put the JSON data into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "artwork_ids = [df['contentId']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54952299-0ae3-4713-b8da-c05de6eca6a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "This JSON data includes lots of useful information, but doesn't include the natural-language description of the artworks. Each artwork desciption is found on the artwork's individual page, the content of which is also accessible in JSON format through WIkiArt's API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b02439f-2cc5-4b6d-8e54-b6da2c098055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty Python dictionary into which to put the descriptions\n",
    "artwork_description_dict = {}\n",
    "\n",
    "# Use a for-loop to access each individual artwork page and get each artwork description. \n",
    "for artwork_id in artwork_ids[0]:\n",
    "    start_artwork_url = 'https://www.wikiart.org/en/App/Painting/ImageJson/'\n",
    "    full_artwork_url = start_artwork_url+str(artwork_id)\n",
    "\n",
    "    artwork_response = urlopen(full_artwork_url)\n",
    "    artwork_data = json.loads(artwork_response.read())\n",
    "    description = artwork_data['description']\n",
    "    # Append the artwork description as 'value' in the dictionary, with artwork_id as the key\n",
    "    artwork_description_dict[artwork_id] = description\n",
    "    \n",
    "# Put this information into a pandas DataFrame, which can then be merged with the \n",
    "# rest of the information which is already in a pandas DataFrame\n",
    "descriptions_data = artwork_description_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bc44c47-1004-4689-bc45-212cd4ab6ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn1 = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad61b7f-050f-450d-920a-77a6b493cc89",
   "metadata": {},
   "source": [
    "The descriptions need a bit of processing and cleaning up. I will remove the URLs and tags, convert to lowercase, remove stopwords and lemmatize, using tools from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d23f9063-de75-49ee-86dc-31955922a288",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_descriptions_dict = {}\n",
    "#pattern = re.compile(\"^([A-Z][0-9]+)+$\")\n",
    "pattern = re.compile(r'\\w')\n",
    "url_pattern = r'(url)|(href=.+?])'\n",
    "\n",
    "for artwork_id in artwork_description_dict:\n",
    "    description = artwork_description_dict[artwork_id]\n",
    "    if description is not None:\n",
    "        sents = nltk.sent_tokenize(description)\n",
    "        sents_no_urls = [re.sub(url_pattern, '', sent) for sent in sents]\n",
    "        # list comprehesion with word tokenizer\n",
    "        words = [nltk.word_tokenize(sent) for sent in sents_no_urls]\n",
    "        all_words = [item for sublist in words for item in sublist if not item in stop_words]\n",
    "        words_lower = [word.lower() for word in all_words if pattern.match(word)]\n",
    "        words_lemmatized = [wn1.lemmatize(word) for word in words_lower]\n",
    "        processed_desc = ' '.join(words_lemmatized)\n",
    "        processed_descriptions_dict[artwork_id] = processed_desc\n",
    "        \n",
    "descriptions_list = [(k, v) for k, v in processed_descriptions_dict.items()]\n",
    "description_df = pd.DataFrame(descriptions_list)\n",
    "description_df.columns = ['contentId', 'description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ce86075-0902-4a7c-a679-19591aa58ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>contentId</th>\n",
       "      <th>artistName</th>\n",
       "      <th>image</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mona Lisa</td>\n",
       "      <td>225189</td>\n",
       "      <td>Leonardo da Vinci</td>\n",
       "      <td>https://uploads0.wikiart.org/00339/images/leon...</td>\n",
       "      <td>one iconic recognizable painting world mona li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Starry Night</td>\n",
       "      <td>207190</td>\n",
       "      <td>Vincent van Gogh</td>\n",
       "      <td>https://uploads4.wikiart.org/00142/images/vinc...</td>\n",
       "      <td>van gogh night sky field roiling energy below ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Persistence of Memory</td>\n",
       "      <td>221654</td>\n",
       "      <td>Salvador Dali</td>\n",
       "      <td>https://uploads.wikiart.org/Content/images/FRA...</td>\n",
       "      <td>the persistence memory 1931 one iconic recogni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In Bed, The Kiss</td>\n",
       "      <td>230453</td>\n",
       "      <td>Henri de Toulouse-Lautrec</td>\n",
       "      <td>https://uploads8.wikiart.org/images/henri-de-t...</td>\n",
       "      <td>this captivating 1892 artwork in bed the kiss ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Birth of Venus</td>\n",
       "      <td>189114</td>\n",
       "      <td>Sandro Botticelli</td>\n",
       "      <td>https://uploads6.wikiart.org/images/sandro-bot...</td>\n",
       "      <td>the birth venus painted sandro botticelli 1480...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       title  contentId                 artistName  \\\n",
       "0                  Mona Lisa     225189          Leonardo da Vinci   \n",
       "1           The Starry Night     207190           Vincent van Gogh   \n",
       "2  The Persistence of Memory     221654              Salvador Dali   \n",
       "3           In Bed, The Kiss     230453  Henri de Toulouse-Lautrec   \n",
       "4         The Birth of Venus     189114          Sandro Botticelli   \n",
       "\n",
       "                                               image  \\\n",
       "0  https://uploads0.wikiart.org/00339/images/leon...   \n",
       "1  https://uploads4.wikiart.org/00142/images/vinc...   \n",
       "2  https://uploads.wikiart.org/Content/images/FRA...   \n",
       "3  https://uploads8.wikiart.org/images/henri-de-t...   \n",
       "4  https://uploads6.wikiart.org/images/sandro-bot...   \n",
       "\n",
       "                                         description  \n",
       "0  one iconic recognizable painting world mona li...  \n",
       "1  van gogh night sky field roiling energy below ...  \n",
       "2  the persistence memory 1931 one iconic recogni...  \n",
       "3  this captivating 1892 artwork in bed the kiss ...  \n",
       "4  the birth venus painted sandro botticelli 1480...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the dataframes\n",
    "complete_df = df.merge(description_df, on='contentId', how='left', sort=False)\n",
    "complete_df.replace('None', np.NaN, inplace=True)\n",
    "complete_df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)\n",
    "database_df = complete_df[['title', 'contentId', 'artistName', 'image', 'description']].copy()\n",
    "database_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80f17727-06f9-4749-81b1-45dbc2833c0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add artwork data to Whoosh index \n",
    "\n",
    "schema = Schema(title=TEXT(stored=True, analyzer=StemmingAnalyzer()), \n",
    "                contentId=ID(stored=True),\n",
    "                artistName=TEXT(stored=True),\n",
    "                imageURL=ID(stored=True),\n",
    "                description=TEXT(stored=True, analyzer=StemmingAnalyzer()))\n",
    "\n",
    "if not os.path.exists(search_engine_index_path):\n",
    "    os.mkdir(search_engine_index_path)\n",
    "\n",
    "ix = index.create_in(search_engine_index_path, schema)\n",
    "\n",
    "with ix.writer() as writer:\n",
    "    for i in range(len(database_df)):\n",
    "        writer.add_document(title=str(database_df.title.iloc[i]), \n",
    "                            contentId=str(database_df.contentId.iloc[i]), \n",
    "                            artistName=str(database_df.artistName.iloc[i]), \n",
    "                            imageURL=str(database_df.image.iloc[i]), \n",
    "                            description=str(database_df.description.iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6e58776-5f33-4798-b47f-dfec700559ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/19477319/whoosh-accessing-search-page-result-items-throws-readerclosed-exception\n",
    "# http://annamarbut.blogspot.com/2018/08/whoosh-pandas-and-redshift-implementing.html\n",
    "# https://ai.intelligentonlinetools.com/ml/search-text-documents-whoosh/\n",
    "def index_search(search_query):\n",
    "    \n",
    "    print(f\"You entered '{search_query}'.\")\n",
    "    \n",
    "    ix = index.open_dir('whoosh_artwork_index')\n",
    "    schema = ix.schema\n",
    "    \n",
    "    try:\n",
    "        other_words = get_similar_words(model, search_query)\n",
    "        final_search_query = ' OR '.join([search_query] + other_words)\n",
    "        print('Synonyms: {}\\n'.format(other_words))\n",
    "    except:\n",
    "        final_search_query = search_query\n",
    "        print('No synonyms available for your search term.')\n",
    "    \n",
    "    q = MultifieldParser(['title', 'description'], schema).parse(final_search_query)\n",
    "    \n",
    "    results = ix.searcher().search(q)\n",
    "    \n",
    "    if len(results)==0:\n",
    "        print('\\nSorry, there are no good matches for your search term.\\nWould you like to try entering something different?')\n",
    "        top_image = np.NaN\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        top_image_url = results[0]['imageURL']\n",
    "        top_artist = results[0]['artistName']\n",
    "        top_title = results[0]['title']\n",
    "        try:\n",
    "            for i, result in enumerate(results):\n",
    "                artist = result['artistName']\n",
    "                title = result['title']\n",
    "                #description = result['description']\n",
    "                #contentId = result['contentId']    \n",
    "                #image_url = result['imageURL']\n",
    "                print(i+1, ': \\'{}\\' by {}'.format(title, artist))\n",
    "                \n",
    "            if str(top_image_url)=='https://uploads.wikiart.org/Content/images/FRAME-600x480.jpg':\n",
    "                try:\n",
    "                    top_image_url = 'https://www.google.com/search?tbm=isch&q=find'+str(top_artist)+str(top_title)\n",
    "                    top_image = Image.open(requests.get(top_image_url, stream=True).raw)\n",
    "                    display(top_image)\n",
    "                except:\n",
    "                    print('\\nSorry, no image is available for your top artwork.')\n",
    "                    pass\n",
    "                \n",
    "                try:\n",
    "                    for j in range(len(results)):\n",
    "                        backup_image_url = results[j+1]['imageURL']\n",
    "                        backup_image = Image.open(requests.get(backup_image_url, stream=True).raw)\n",
    "                        if str(backup_image_url)=='https://uploads.wikiart.org/Content/images/FRAME-600x480.jpg':\n",
    "                            j+=1\n",
    "                        else:\n",
    "                            print('\\nHere\\'s an image of your number {} result:'.format(j+2)) \n",
    "                            display(backup_image)\n",
    "                            break\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "                return top_artist, top_title\n",
    "                \n",
    "            else:\n",
    "                print('\\nHere\\'s an image of your top result:') \n",
    "                top_image = Image.open(requests.get(top_image_url, stream=True).raw)\n",
    "                display(top_image)\n",
    "                return artist, title\n",
    "            print(\"\\nTop search result(s) for '{}':\".format(search_query))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print('\\nSorry, an error occurred.')\n",
    "            print(e)\n",
    "        pass        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4fb6fe-3b95-41e2-a90a-286fcebc71d3",
   "metadata": {},
   "source": [
    "# Artwork search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8cd141e-1474-41a1-9b21-6045a5ae5c09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24820ca06cd8443aac1e247cb435a777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Enter your search term to see the best-matching artwork:'), Text(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_engine_prompt = widgets.Label(value='Enter your search term to see the best-matching artwork:')\n",
    "text_input = widgets.Text(value='', placeholder='Type something', disabled=False, continuous_update=False)\n",
    "search_engine_input = widgets.HBox([search_engine_prompt, text_input])\n",
    "\n",
    "def f(query):\n",
    "    return index_search(query)\n",
    "        \n",
    "out = widgets.interactive_output(f, {'query': text_input})\n",
    "\n",
    "widgets.VBox([search_engine_input, out])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_semantic_artwork_search",
   "language": "python",
   "name": "nlp_semantic_artwork_search"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
